{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade vidore_benchmark==4.0.2\n",
    "!pip install --upgrade pymupdf\n",
    "!pip install openpyxl\n",
    "! sudo apt install tesseract-ocr -y\n",
    "!pip install pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('models/colqwen2-visrag-checkpoint-6000/adapter_config.json') as f:\n",
    "#     ad = json.load(f)\n",
    "# base = ad['base_model_name_or_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vidore_benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoProcessor\n",
    "from vidore_benchmark.retrievers.vision_retriever import VisionRetriever\n",
    "\n",
    "import torch\n",
    "from typing import List, Optional, Tuple, Union, TypeVar\n",
    "\n",
    "from __future__ import annotations\n",
    "import logging\n",
    "from typing import ClassVar, List, Optional, Union, cast\n",
    "import pandas as pd\n",
    "from colpali_engine.models.qwen2_5.colqwen2_5.modeling_colqwen2_5 import ColQwen2_5\n",
    "from colpali_engine.models.qwen2_5.colqwen2_5.processing_colqwen2_5 import ColQwen2_5_Processor\n",
    "from colpali_engine.utils.torch_utils import get_torch_device\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class ListDataset(Dataset[T]):\n",
    "    def __init__(self, elements: List[T]):\n",
    "        self.elements = elements\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.elements)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> T:\n",
    "        return self.elements[idx]\n",
    "\n",
    "\n",
    "def averaged_st(models_list):\n",
    "    \n",
    "    state_dicts = [model.state_dict() for model in models_list]\n",
    "    averaged_st =  models_list[0].state_dict()\n",
    "    \n",
    "    \n",
    "    for key in averaged_st.keys():\n",
    "        for i in range(1, len(state_dicts)):\n",
    "            averaged_st[key] += state_dicts[i][key]\n",
    "            \n",
    "        averaged_st[key] /= len(state_dicts)\n",
    "        \n",
    "    \n",
    "    return averaged_st\n",
    "\n",
    "\n",
    "def score_multi_vector(\n",
    "        qs: List[torch.Tensor],\n",
    "        ps: List[torch.Tensor],\n",
    "        batch_size: int = 128,\n",
    "        device: Optional[Union[str, torch.device]] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the MaxSim score (ColBERT-like) for the given multi-vector query and passage embeddings.\n",
    "        \"\"\"\n",
    "        device = device or get_torch_device(\"auto\")\n",
    "\n",
    "        if len(qs) == 0:\n",
    "            raise ValueError(\"No queries provided\")\n",
    "        if len(ps) == 0:\n",
    "            raise ValueError(\"No passages provided\")\n",
    "\n",
    "        scores_list: List[torch.Tensor] = []\n",
    "\n",
    "        for i in range(0, len(qs), batch_size):\n",
    "            scores_batch = []\n",
    "            qs_batch = torch.nn.utils.rnn.pad_sequence(qs[i : i + batch_size], batch_first=True, padding_value=0).to(\n",
    "                device\n",
    "            )\n",
    "            for j in range(0, len(ps), batch_size):\n",
    "                ps_batch = torch.nn.utils.rnn.pad_sequence(\n",
    "                    ps[j : j + batch_size], batch_first=True, padding_value=0\n",
    "                ).to(device)\n",
    "                scores_batch.append(torch.einsum(\"bnd,csd->bcns\", qs_batch, ps_batch).max(dim=3)[0].sum(dim=2))\n",
    "            scores_batch = torch.cat(scores_batch, dim=1).cpu()\n",
    "            scores_list.append(scores_batch)\n",
    "\n",
    "        scores = torch.cat(scores_list, dim=0)\n",
    "        assert scores.shape[0] == len(qs), f\"Expected {len(qs)} scores, got {scores.shape[0]}\"\n",
    "\n",
    "        scores = scores.to(torch.float32)\n",
    "        return scores\n",
    "\n",
    "\n",
    "\n",
    "#from vidore_benchmark.retrievers.registry_utils import register_vision_retriever\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "#@register_vision_retriever(\"colqwen2\")\n",
    "class ColQwenRetriever(VisionRetriever):\n",
    "    \"\"\"\n",
    "    ColPali Retriever that implements the model from \"ColPali: Efficient Document Retrieval\n",
    "    with Vision Language Models\".\n",
    "    \"\"\"\n",
    "\n",
    "    emb_dim_query: ClassVar[int] = 128\n",
    "    emb_dim_doc: ClassVar[int] = 128\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name_or_path,\n",
    "        device: str = \"auto\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = get_torch_device(device)\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "    \n",
    "        # Load the model and LORA adapter\n",
    "        self.model = cast(\n",
    "            ColQwen2_5,\n",
    "            ColQwen2_5.from_pretrained(\n",
    "                pretrained_model_name_or_path,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"cuda\",\n",
    "                attn_implementation= 'flash_attention_2',\n",
    "            ).eval().to(\"cuda\"),\n",
    "        )\n",
    "    \n",
    "\n",
    "            # Load the processor\n",
    "        if \"checkpoint\" in pretrained_model_name_or_path:\n",
    "            pretrained_model_name_or_path = \"/\".join(pretrained_model_name_or_path.split(\"/\")[:-1])\n",
    "        self.processor = cast(ColQwen2_5_Processor, ColQwen2_5_Processor.from_pretrained(pretrained_model_name_or_path))\n",
    "        print(\"Loaded custom processor.\\n\")\n",
    "\n",
    "    @property\n",
    "    def use_visual_embedding(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def process_images(self, images: List[Image.Image], **kwargs):\n",
    "        return self.processor.process_images(images=images)\n",
    "\n",
    "    def process_queries(self, queries: List[str], **kwargs):\n",
    "        return self.processor.process_queries(queries=queries)\n",
    "\n",
    "    def forward_queries(self, queries: List[str], batch_size: int, **kwargs) -> List[torch.Tensor]:\n",
    "        dataloader = DataLoader(\n",
    "            dataset=ListDataset[str](queries),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.process_queries,\n",
    "            num_workers=8\n",
    "        )\n",
    "\n",
    "        qs = []\n",
    "        for batch_query in tqdm(dataloader, desc=\"Forward pass queries...\", leave=False):\n",
    "            with torch.no_grad():\n",
    "                batch_query = {k: v.to(self.device) for k, v in batch_query.items()}\n",
    "                embeddings_query = self.model(**batch_query)\n",
    "                qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n",
    "\n",
    "        return qs\n",
    "\n",
    "    def forward_documents(self, passages: List[Image.Image], batch_size: int, **kwargs) -> List[torch.Tensor]:\n",
    "        dataloader = DataLoader(\n",
    "            dataset=ListDataset[Image.Image](passages),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.process_images,\n",
    "            \n",
    "        )\n",
    "\n",
    "        ds = []\n",
    "        for batch_doc in tqdm(dataloader, desc=\"Forward pass documents...\", leave=False):\n",
    "            with torch.no_grad():\n",
    "                batch_doc = {k: v.to(self.device) for k, v in batch_doc.items()}\n",
    "                embeddings_doc = self.model(**batch_doc)\n",
    "            ds.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n",
    "        return ds\n",
    "\n",
    "    def get_scores(\n",
    "        self,\n",
    "        query_embeddings: Union[torch.Tensor, List[torch.Tensor]],\n",
    "        passage_embeddings: Union[torch.Tensor, List[torch.Tensor]],\n",
    "        batch_size: Optional[int] = 128,\n",
    "    ) -> torch.Tensor:\n",
    "        if batch_size is None:\n",
    "            raise ValueError(\"`batch_size` must be provided for ColPaliRetriever's scoring\")\n",
    "        scores = score_multi_vector(query_embeddings, passage_embeddings, batch_size=batch_size,device=self.device)\n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from vidore_benchmark.evaluation.evaluate import evaluate_dataset\n",
    "# from vidore_benchmark.retrievers.colqwen_retriever import ColQwenRetriever\n",
    "\n",
    "\n",
    "load_dotenv(override=False)\n",
    "\n",
    "def eval_model(model_path,dataset_path):\n",
    "    \"\"\"\n",
    "    Example script for a Python usage of the Vidore Benchmark.\n",
    "    \"\"\"\n",
    "    my_retriever = ColQwenRetriever(model_path)\n",
    "    dataset = load_dataset(dataset_path, split=\"test\")\n",
    "    metrics = evaluate_dataset(my_retriever, dataset, batch_query=32,batch_doc=128,batch_score=128)\n",
    "    return  metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIDORE BEnchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = ['vidore/colqwen2-v0.1+vidore/colqwen2-v1.0',\n",
    "#                'vidore/colqwen2-v0.1+models/colqwen2-visrag',\n",
    "#                'vidore/colqwen2-v0.1+models/colqwen2-mixed',\n",
    "#                 'models/colqwen2-visrag+models/colqwen2-mixed',\n",
    "#                 'vidore/colqwen2-v0.1',\n",
    "#                 'vidore/colqwen2-v1.0',\n",
    "#                 'models/colqwen2-visrag',\n",
    "#                 'models/colqwen2-mixed']\n",
    "\n",
    "model_names = [\n",
    "    './models/colqwen2.5-clipped-part2_lora128_bsz128x1_lr2e-4/checkpoint-1300',\n",
    "    './models/colqwen2.5-clipped-part2_lora128_bsz128x1_lr2e-4/checkpoint-1400',\n",
    "    './models/colqwen2.5-clipped-part2_lora128_bsz128x1_lr2e-4/checkpoint-1500',\n",
    "    './models/colqwen2.5-clipped-part2_lora128_bsz128x1_lr2e-4/checkpoint-1700',\n",
    "    './models/colqwen2.5-clipped-part2_lora128_bsz128x1_lr2e-4/checkpoint-1900',\n",
    "]\n",
    "\n",
    "\n",
    "dataset_names = [\n",
    "    'vidore/arxivqa_test_subsampled',\n",
    "    'vidore/docvqa_test_subsampled',\n",
    "    'vidore/infovqa_test_subsampled', \n",
    "    'vidore/tabfquad_test_subsampled',\n",
    "    'vidore/tatdqa_test',\n",
    "    'vidore/shiftproject_test',\n",
    "    'vidore/syntheticDocQA_artificial_intelligence_test',\n",
    "    'vidore/syntheticDocQA_energy_test',\n",
    "    'vidore/syntheticDocQA_government_reports_test',\n",
    "    'vidore/syntheticDocQA_healthcare_industry_test'\n",
    "]\n",
    "score_df = pd.DataFrame(index = model_names,columns=[i.split('/')[1] for i in dataset_names])\n",
    "\n",
    "\n",
    "for data in dataset_names:\n",
    "    for model_name in model_names:\n",
    "        metrics = eval_model(model_name,data)\n",
    "        score_df.loc[model_name,data.split('/')[1]] = metrics['ndcg_at_5']\n",
    "        score_df.to_excel('./results.xlsx')\n",
    "print(score_df.mean().mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISRAG BEnchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['vidore/colqwen2-v0.1','models/colqwen2-mixed','models/colqwen2-visrag']\n",
    "\n",
    "dataset_names = ['Metric-AI/VisRAG-Ret-Test-ChartQA','Metric-AI/VisRAG-Ret-Test-ArxivQA',\n",
    "                 'Metric-AI/VisRAG-Ret-Test-MP-DocVQA','Metric-AI/VisRAG-Ret-Test-InfoVQA',\n",
    "                 'Metric-AI/VisRAG-Ret-Test-PlotQA','Metric-AI/VisRAG-Ret-Test-SlideVQA']\n",
    "\n",
    "score_df = pd.DataFrame(index = model_names,columns=[i.split('/')[1] for i in dataset_names])\n",
    "\n",
    "\n",
    "for data in dataset_names:\n",
    "    for model_name in model_names:\n",
    "        metrics = eval_model(model_name,data)\n",
    "        score_df.loc[model_name,data.split('/')[1]] = metrics['ndcg_at_5']\n",
    "        score_df.to_excel('Visrag_bench.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colpali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
